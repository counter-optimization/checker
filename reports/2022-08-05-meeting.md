---
title: Report on small bitwidth checker on fmul2 compiled with O0
date: 2022-08-04
---

# checker

* finished looking over comp simp results for curve25519, f51, fmul2,
  at -O0:
  - this is the function we were looking at with paul beame
  - took *6min13 seconds to analyze*. 
  - with my bare metal racket analyzer working on this C code, this
    was analyzed in *4 seconds*
	* not to say we should switch to anything else, but this is not
      very fast. 
	* i think maybe 40% my code inefficiencies from copying too much
      state data around for recording, 60% angr inefficiencies
	  - mostly that there is no fine-grained hook for
        analyzing binops/triops/quadops)
	  - for example, if each operand in an expr is an AST, then angr
        will visit each node of the AST. as the ASTs accumulate across
        the dependent instructions in fmul2, polynomially many
        ASTs are checked when it should probably be linear in
        the num of insns
	* im also running pypy which i was hoping would make this faster
  - overall, the checker analyzed 2267 expressions for comp simp
    safety
  - **i hand verified that the 2267 analyzed instructions with 2192 not
    being comp simplifiable and 75 being comp simplifiable are exactly
    the expected results**:
	* all spots i thought should be hardened by manually analyzing the
      fmul2 asm are in the set the checker says needs to be hardended
	* all spots i thought should *not* be hardended, the checker says
      does not need to be hardended
	* all spots the checker reported need to be hardned are in the set
      i thought should be hardended
	* i did not hand analyze all of the 2016 (see below) stack operations
      that the checker says do *not* need hardening, but instead checked
      that they are all add64,sub64 of const rsp/rbp (left operand)
      and const offset (right operand)
	
  - simplifiable stats:
    * **there were 75 total exprs flagged by the checker, and i think 67
      of these would need instrumenting**
	* not occurring in real binary (8/75):
	  - an insn used to calculate flags (explicit in vexir) can be
        comp simpd.
	  - the correct insn to worry about was flagged for that addr tho
	  - i have examples in a jupyter notebook
	* the rest (67/75):
	  - for ECC, correctly identified that all of the actual 'ECC
        logic' needs to be hardened against comp simp
	  - these are the assembly instructions that you think would be
        generated just from a quick skim of the fmul2 `C`
        [code](https://github.com/project-everest/hacl-star/blob/b1dc1d7acda65d3c469a64c42895adee48a59826/dist/linux/Hacl_Bignum25519_51.h#L156
        "link to fmul2 in curve25519 field51 bignum arith") 
	  - a good chunk of these are arithmetic insns using LEA. all of
        the multiplications by 19 are implemented through two LEA
        instructions (this is O0)
		
  - non-simplifiable stats:
    * **there were 2192 total exprs the checker deemed not
      simplifiable** 
	
	* push/pop ops (161/2192):
	  - these are from the add/sub part of a push/pop after lifting
        into VEXIR
      - probably would not be instrumented, so no savings.
	  - it would be difficult to tell in the VEX IR that this is a
        push or pop (see code excerpt below)
		
    * explicit add or sub of rsp/rbp for a load (2008/2192):
	  - heuristic: left operand is a constant bitvector that starts
        with `<BV64 0x7fffffffff`: e.g., `<BV64 0x7fffffffffefdf0>`
	  - these are all of one of either forms:
	    * Add64(const_stack_addr, const_offs)
		* Sub64(const_stack_addr, const_offs)
	  - example insns (where the rbp - N) is what is flagged in the
        VEX IR
		* `mov qword ptr [rbp - 8], rdi`
		* `mov qword ptr [rbp - 16], rsi`
		
	* pointer arith on pointer args to functions (8/2192):
	  - these are the load/store instructions on the pointers passed
        in to fmul2 that point to the 5 uint64_t limbs for each point
		
	* misc (15/2192):
	  - operations on constants (e.g., 0x33 & 0x3f) that would
	    probably be simplified by compiler about `O0`
		* mostly operations on field numbers (e.g., mask by 51). i
          don't think they would be simplifiable, and they would only
          leak whether the curve was in field 51 or 64
	  - of these 15:
	    * 6/15 are actually appear in the code
		* the other 9/15 are only in the VEX IR used for calculating
          flag variables and don't actually appear in the code

**i am worried about tying the VEX IR instructions that the checker
alerts on back to the assembly the compiler sees.**
as an example,
```
shrd	rax, rdx, cl
```
gets lifted to this VEX IR:
```
-- VEX IR INSN MARK STATEMENT: START OF INSN AT 0x40396b --
   127 | t20 = GET:I64(rdx)
   128 | t21 = GET:I64(rax)
   129 | t101 = GET:I8(cl)
   130 | t100 = And8(t101,0x3f)
   131 | t22 = t100
   132 | t103 = Sub8(t22,0x01)
   133 | t102 = And8(t103,0x3f)
   134 | t23 = t102
   135 | t105 = CmpNE8(t22,0x00)
   136 | t107 = Shr64(t21,t22)
   137 | t109 = Sub8(0x40,t22)
   138 | t108 = Shl64(t20,t109)
   139 | t106 = Or64(t107,t108)
   140 | t104 = ITE(t105,t106,t21)
   141 | t25 = t104
   142 | t111 = CmpNE8(t23,0x00)
   143 | t113 = Shr64(t21,t23)
   144 | t115 = Sub8(0x40,t23)
   145 | t114 = Shl64(t20,t115)
   146 | t112 = Or64(t113,t114)
   147 | t110 = ITE(t111,t112,t21)
   148 | t26 = t110
   149 | t27 = t25
   150 | t28 = t26
   151 | t29 = CmpNE8(t22,0x00)
   152 | t117 = GET:I64(cc_op)
   153 | t116 = ITE(t29,0x0000000000000024,t117)
   154 | PUT(cc_op) = t116
   155 | t119 = GET:I64(cc_dep1)
   156 | t118 = ITE(t29,t27,t119)
   157 | PUT(cc_dep1) = t118
   158 | t121 = GET:I64(cc_dep2)
   159 | t120 = ITE(t29,t28,t121)
   160 | PUT(cc_dep2) = t120
   161 | PUT(rax) = t27
-- VEX IR INSN MARK STATEMENT: START OF [next_insn] --
```
here, since the result should be in `rax`, only `t27` and its
dependent subexpressions are important to the transformer:
```
-- VEX IR INSN MARK STATEMENT: START OF INSN AT 0x40396b --
127 | t20 = GET:I64(rdx)
128 | t21 = GET:I64(rax)
129 | t101 = GET:I8(cl)
130 | t100 = And8(t101,0x3f)
131 | t22 = t100
...
135 | t105 = CmpNE8(t22,0x00)
136 | t107 = Shr64(t21,t22)
137 | t109 = Sub8(0x40,t22)
138 | t108 = Shl64(t20,t109)
139 | t106 = Or64(t107,t108)
140 | t104 = ITE(t105,t106,t21)
141 | t25 = t104
...
149 | t27 = t25
...
161 | PUT(rax) = t27
-- VEX IR INSN MARK STATEMENT: START OF [next_insn] --
```
If we know that this specific slice of the lifted instruction is
important, then any subexpression of this slice being comp simpable
means the assembly instruction needs to be transformed. 

If we know that the rest of the lifted vex IR for this insn is only
used for flags, then we can ignore if they alert.

Angr does not have an out-of-the-box way to tell which part of the
lifted IR does what in the original assembly instruction.

**To make life simpler, probably just always flag an assembly insn for
transforming if any part of its lifted VEX IR is flagged by the
checker.**

By cases:
1. (the 'desired slice' flags, the 'flag calc part' flags) -> safe
2. (the 'desired slice' doesn't flag, the 'flag calc part' flags) ->
overapprox/false pos
3. (the 'desired slice' flags, the 'flag calc part' flags) -> safe
4. neither flag -> safe to ignore

since the flag part is calculated off of the 'desired slice' part of
the VEX IR, i feel like 'flag calc part' flags implies that the
'desired slice' part flags anyways. if not, i would then think that
the overapprox case would be pretty rare

the double precision `shr` is just one example. the normal `shr`,
`test`, `cmovcc` insns are also pretty messy like this. i guess this
would be the case for any insn that turns into many micro-ops?

all this talk about false positives makes me think that people reading
the paper might want [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) numbers for the
approximating checkers like other program analysis papers.
 
* tying all of these instructions together brings back the flow
  question:
  
# flow

* there is a chicken-and-egg situation with needing the binary to
  check, checking the binary, and then sending the info back to the
  compiler for generating the new, safe binary:
  - checker depends on a binary
  - safe transformation depends on checker results
  - check of safe transformation final code depends on a transformed
    binary 
	
* we want the most ergonomic flow with these dependencies

* i think it will be easier to *not* harden each spot to begin with
  with the checker then removing the unnecessary spots
  * the removal means adding the old or an equivalent instruction
    back in which is probably not simple to do in the checker. if not
    in the checker then we're going to round trip the compiler/bin
    rewriter again anyways

the first tool flow that comes to mind:
1. all functions needing checking are marked in some way
2. all uarch side channels the code needs to be hardened against are
   marked in some way
3. code is compiled as usual producing binary B
4. a list of functions that need to be checked are sent to the checker
  * what is the format:
    - dan: debug info
	- could always output just a txt list or json
5. for each function F needing hardened in B: run checker on F 
  * need to ident F
  * need to harden all functions that F calls
  * the analysis results on the binary should be for the same binary
	that the compiler would output the next time the same source files
	are compiled
	- is this safe enough?
	  * could break if compiler or compiler versions are switched
        somewhere in the flow, so this whole flow should be atomic
        from the user's viewpoint: Makefile/CMake/autoconf ran???
6. checker flags all instruction offsets for func F that need to be
   hardened with uarch mitigation M
7. the code is recompiled using the results from 6
8. repeat step 5: no insns should be flagged as potentially vulnerable

